{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import scipy.signal\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from general import get_logger, Progbar, export_plot\n",
    "from config import config\n",
    "from test_env import EnvAdd, EnvAdd2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "          mlp_input, \n",
    "          output_size,\n",
    "          scope, \n",
    "          n_layers=config.n_layers, \n",
    "          size=config.layer_size, \n",
    "          output_activation=None):\n",
    "  with tf.variable_scope(scope, reuse = False):\n",
    "    cur_input = mlp_input\n",
    "    #cur_input = tf.contrib.layers.fully_connected(cur_input, size)\n",
    "    for i in range(n_layers):\n",
    "      cur_input = tf.contrib.layers.fully_connected(cur_input,size)\n",
    "    output = tf.contrib.layers.fully_connected(cur_input,output_size, activation_fn=output_activation)\n",
    "  return output\n",
    "\n",
    "class PG(object):\n",
    "  def __init__(self, env, config, logger=None):\n",
    "    \n",
    "    if not os.path.exists(config.output_path):\n",
    "      os.makedirs(config.output_path)\n",
    "    self.config = config\n",
    "    self.env = env\n",
    "    self.logger = logger\n",
    "    if logger is None:\n",
    "      self.logger = get_logger(config.log_path)\n",
    "    self.env = env\n",
    "  \n",
    "    self.discrete = True\n",
    "    #self.observation_dim = self.env.observation_space.shape[0]\n",
    "    self.observation_dim = 4\n",
    "    #self.action_dim = self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "    self.action_dim = 4\n",
    "  \n",
    "    self.lr = self.config.learning_rate\n",
    "  \n",
    "    # build model\n",
    "    self.build()\n",
    "  \n",
    "  \n",
    "  def add_placeholders_op(self):\n",
    "    self.observation_placeholder = tf.placeholder(tf.float32,(None,self.observation_dim))\n",
    "    if self.discrete:\n",
    "      self.action_placeholder = tf.placeholder(tf.int32)\n",
    "    else:\n",
    "      self.action_placeholder = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Define a placeholder for advantages\n",
    "    self.advantage_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "  \n",
    "  \n",
    "  def build_policy_network_op(self, scope = \"policy_network\"):\n",
    "\n",
    "    if self.discrete:\n",
    "      action_logits = build_mlp(mlp_input=self.observation_placeholder, output_size=self.action_dim, scope= scope) \n",
    "      self.sampled_action = tf.squeeze(tf.multinomial(action_logits,1),axis=[1])\n",
    "      #self.sampled_action = tf.multinomial(action_logits,1)\n",
    "      #action_taken = tf.one_hot(self.action_placeholder, depth=self.action_dim, axis=-1, dtype = tf.int32)\n",
    "      cross_ent_neg = (-1)*tf.nn.sparse_softmax_cross_entropy_with_logits(logits= action_logits, labels= self.action_placeholder)\n",
    "      #action_taken = tf.cast(action_taken,tf.float32)\n",
    "      self.logprob = cross_ent_neg\n",
    "    else:\n",
    "      action_means =  build_mlp(self.observation_placeholder, self.action_dim, scope=scope)\n",
    "      #with tf.variable_scope(\"log_std\", reuse=tf.AUTO_REUSE):\n",
    "      log_std = tf.get_variable(\"log_std\",[self.action_dim])\n",
    "      std = tf.exp(log_std)\n",
    "      self.sampled_action = action_means+ std*tf.random_normal([self.action_dim])\n",
    "      density = tf.contrib.distributions.MultivariateNormalDiag(loc=action_means, scale_diag=std)\n",
    "      self.logprob = tf.log(density.prob(self.action_placeholder))\n",
    "            \n",
    "  \n",
    "  \n",
    "  def add_loss_op(self):\n",
    "\n",
    "    self.loss = -tf.reduce_mean(tf.multiply(self.logprob,self.advantage_placeholder))\n",
    "\n",
    "  \n",
    "  \n",
    "  def add_optimizer_op(self):\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "    self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "  \n",
    "  \n",
    "  def add_baseline_op(self, scope = \"baseline\"):\n",
    "\n",
    "    self.baseline = build_mlp(self.observation_placeholder,1,scope=scope)\n",
    "    self.baseline_target_placeholder = tf.placeholder(tf.float32)\n",
    "    baseline_loss = tf.losses.mean_squared_error(self.baseline_target_placeholder, self.baseline)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "    self.update_baseline_op = optimizer.minimize(baseline_loss)\n",
    "\n",
    "  \n",
    "  def build(self):\n",
    "\n",
    "    # add placeholders\n",
    "    self.add_placeholders_op()\n",
    "    # create policy net\n",
    "    self.build_policy_network_op()\n",
    "    # add square loss\n",
    "    self.add_loss_op()\n",
    "    # add optmizer for the main networks\n",
    "    self.add_optimizer_op()\n",
    "  \n",
    "    if self.config.use_baseline:\n",
    "      self.add_baseline_op()\n",
    "  \n",
    "  def initialize(self):\n",
    "\n",
    "    # create tf session\n",
    "    self.sess = tf.Session()\n",
    "    # tensorboard stuff\n",
    "    self.add_summary()\n",
    "    # initiliaze all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    self.sess.run(init)\n",
    "  \n",
    "  \n",
    "  def add_summary(self):\n",
    "\n",
    "    # extra placeholders to log stuff from python\n",
    "    self.avg_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"avg_reward\")\n",
    "    self.max_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"max_reward\")\n",
    "    self.std_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"std_reward\")\n",
    "  \n",
    "    self.eval_reward_placeholder = tf.placeholder(tf.float32, shape=(), name=\"eval_reward\")\n",
    "  \n",
    "    # extra summaries from python -> placeholders\n",
    "    tf.summary.scalar(\"Avg Reward\", self.avg_reward_placeholder)\n",
    "    tf.summary.scalar(\"Max Reward\", self.max_reward_placeholder)\n",
    "    tf.summary.scalar(\"Std Reward\", self.std_reward_placeholder)\n",
    "    tf.summary.scalar(\"Eval Reward\", self.eval_reward_placeholder)\n",
    "            \n",
    "    # logging\n",
    "    self.merged = tf.summary.merge_all()\n",
    "    self.file_writer = tf.summary.FileWriter(self.config.output_path,self.sess.graph) \n",
    "\n",
    "  def init_averages(self):\n",
    "\n",
    "    self.avg_reward = 0.\n",
    "    self.max_reward = 0.\n",
    "    self.std_reward = 0.\n",
    "    self.eval_reward = 0.\n",
    "  \n",
    "\n",
    "  def update_averages(self, rewards, scores_eval):\n",
    "\n",
    "    self.avg_reward = np.mean(rewards)\n",
    "    self.max_reward = np.max(rewards)\n",
    "    self.std_reward = np.sqrt(np.var(rewards) / (len(rewards)+0.000000001))\n",
    "  \n",
    "    if len(scores_eval) > 0:\n",
    "      self.eval_reward = scores_eval[-1]\n",
    "  \n",
    "  \n",
    "  def record_summary(self, t):\n",
    "\n",
    "    fd = {\n",
    "      self.avg_reward_placeholder: self.avg_reward, \n",
    "      self.max_reward_placeholder: self.max_reward, \n",
    "      self.std_reward_placeholder: self.std_reward, \n",
    "      self.eval_reward_placeholder: self.eval_reward, \n",
    "    }\n",
    "    summary = self.sess.run(self.merged, feed_dict=fd)\n",
    "    # tensorboard stuff\n",
    "    self.file_writer.add_summary(summary, t)\n",
    "  \n",
    "  \n",
    "  def sample_path(self, env, num_episodes = None):\n",
    "\n",
    "    episode = 0\n",
    "    episode_rewards = []\n",
    "    paths = []\n",
    "    t = 0\n",
    "  \n",
    "    while (num_episodes or t < self.config.batch_size):\n",
    "      state = env.reset()\n",
    "      states, actions, rewards = [], [], []\n",
    "      episode_reward = 0\n",
    "  \n",
    "      for step in range(self.config.max_ep_len):\n",
    "        states.append(state)\n",
    "        action = self.sess.run(self.sampled_action, feed_dict={self.observation_placeholder : states[-1][None]})[0]\n",
    "        state, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        t += 1\n",
    "        if (done or step == self.config.max_ep_len-1):\n",
    "          episode_rewards.append(episode_reward)  \n",
    "          break\n",
    "        if (not num_episodes) and t == self.config.batch_size:\n",
    "          break\n",
    "  \n",
    "      path = {\"observation\" : np.array(states), \n",
    "                      \"reward\" : np.array(rewards), \n",
    "                      \"action\" : np.array(actions)}\n",
    "      paths.append(path)\n",
    "      episode += 1\n",
    "      if num_episodes and episode >= num_episodes:\n",
    "        break        \n",
    "  \n",
    "    return paths, episode_rewards\n",
    "  \n",
    "  \n",
    "  def get_returns(self, paths):\n",
    "\n",
    "\n",
    "    all_returns = []\n",
    "    for path in paths:\n",
    "      rewards = path[\"reward\"]\n",
    "      path_returns = np.zeros(len(rewards))\n",
    "      T = len(rewards)\n",
    "      for i in range(T):\n",
    "        G_t = 0\n",
    "        for j in range(T-i):\n",
    "          G_t += np.power(self.config.gamma,j)*rewards[i+j]\n",
    "        path_returns[i] = G_t\n",
    "      all_returns.append(path_returns)\n",
    "    returns = np.concatenate(all_returns)\n",
    "  \n",
    "    return returns\n",
    "  \n",
    "  \n",
    "  def calculate_advantage(self, returns, observations):\n",
    "\n",
    "    adv = returns\n",
    "    if self.config.use_baseline:\n",
    "      baseline = self.sess.run(self.baseline, feed_dict={self.observation_placeholder : observations})\n",
    "      adv = adv - baseline\n",
    "    if self.config.normalize_advantage:\n",
    "      mean=np.mean(adv)\n",
    "      std = np.std(adv)\n",
    "      adv = (adv - mean)/(std+0.0000001)\n",
    "    return adv\n",
    "  \n",
    "  \n",
    "  def update_baseline(self, returns, observations):\n",
    "\n",
    "    self.sess.run(self.update_baseline_op, feed_dict={self.observation_placeholder : observations, self.baseline_target_placeholder : returns})\n",
    "\n",
    "  \n",
    "  \n",
    "  def train(self):\n",
    "\n",
    "    last_eval = 0 \n",
    "    last_record = 0\n",
    "    scores_eval = []\n",
    "    \n",
    "    self.init_averages()\n",
    "    scores_eval = [] # list of scores computed at iteration time\n",
    "  \n",
    "    for t in range(self.config.num_batches):\n",
    "  \n",
    "      # collect a minibatch of samples\n",
    "      paths, total_rewards = self.sample_path(self.env) \n",
    "      scores_eval = scores_eval + total_rewards\n",
    "      observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "      actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "      rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "      # compute Q-val estimates (discounted future returns) for each time step\n",
    "      returns = self.get_returns(paths)\n",
    "      advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "      # run training operations\n",
    "      if self.config.use_baseline:\n",
    "        self.update_baseline(returns, observations)\n",
    "      self.sess.run(self.train_op, feed_dict={\n",
    "                    self.observation_placeholder : observations, \n",
    "                    self.action_placeholder : actions, \n",
    "                    self.advantage_placeholder : advantages})\n",
    "  \n",
    "      # tf stuff\n",
    "      if (t % self.config.summary_freq == 0):\n",
    "        self.update_averages(total_rewards, scores_eval)\n",
    "        #self.record_summary(t)\n",
    "\n",
    "      # compute reward statistics for this batch and log\n",
    "      avg_reward = np.mean(total_rewards)\n",
    "      sigma_reward = np.sqrt(np.var(total_rewards) / (len(total_rewards)+0.0000001))\n",
    "      msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "      self.logger.info(msg)\n",
    "  \n",
    "      if  self.config.record and (last_record > self.config.record_freq):\n",
    "        self.logger.info(\"Recording...\")\n",
    "        last_record =0\n",
    "        self.record()\n",
    "  \n",
    "    self.logger.info(\"- Training done.\")\n",
    "    export_plot(scores_eval, \"Score\", config.env_name, self.config.plot_output)\n",
    "\n",
    "\n",
    "  def evaluate(self, env=None, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Evaluates the return for num_episodes episodes.\n",
    "    Not used right now, all evaluation statistics are computed during training \n",
    "    episodes.\n",
    "    \"\"\"\n",
    "    if env==None: env = self.env\n",
    "    paths, rewards = self.sample_path(env, num_episodes)\n",
    "    avg_reward = np.mean(rewards)\n",
    "    sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "    msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "    self.logger.info(msg)\n",
    "    return avg_reward\n",
    "     \n",
    "  def produce_example(self):\n",
    "    env = self.env\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    for step in range(self.config.max_ep_len):\n",
    "      states.append(state)\n",
    "      print(\"At step \"+ str(step)+ \" the state is :\" + str(state))\n",
    "      action = self.sess.run(self.sampled_action, feed_dict={self.observation_placeholder : states[-1][None]})[0]\n",
    "      actions = [\"Done\",\"Give A\", \"Give B\", \"Count on\"]\n",
    "      print(\"the action is \" + actions[action])\n",
    "      state, reward, done, info = env.step(action)\n",
    "      if (done or step == self.config.max_ep_len-1):\n",
    "        break\n",
    "\n",
    "  \n",
    "\n",
    "  def run(self):\n",
    "        \n",
    "    self.initialize()\n",
    "\n",
    "    self.train()\n",
    "\n",
    "  def change_env(self,env):\n",
    "    self.env=env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Avg Reward is illegal; using Avg_Reward instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 16:55:56,894] Summary name Avg Reward is illegal; using Avg_Reward instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Max Reward is illegal; using Max_Reward instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 16:55:56,902] Summary name Max Reward is illegal; using Max_Reward instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Std Reward is illegal; using Std_Reward instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 16:55:56,908] Summary name Std Reward is illegal; using Std_Reward instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Eval Reward is illegal; using Eval_Reward instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 16:55:56,913] Summary name Eval Reward is illegal; using Eval_Reward instead.\n",
      "[2018-03-20 16:55:58,978] Average reward: 1.14 +/- 0.39\n",
      "[2018-03-20 16:55:59,313] Average reward: 3.06 +/- 0.47\n",
      "[2018-03-20 16:55:59,669] Average reward: 3.65 +/- 0.42\n",
      "[2018-03-20 16:56:00,027] Average reward: 2.50 +/- 0.33\n",
      "[2018-03-20 16:56:00,398] Average reward: 3.00 +/- 0.36\n",
      "[2018-03-20 16:56:00,748] Average reward: 3.98 +/- 0.43\n",
      "[2018-03-20 16:56:01,091] Average reward: 4.25 +/- 0.44\n",
      "[2018-03-20 16:56:01,433] Average reward: 5.90 +/- 0.52\n",
      "[2018-03-20 16:56:01,764] Average reward: 5.97 +/- 0.52\n",
      "[2018-03-20 16:56:02,103] Average reward: 6.58 +/- 0.54\n",
      "[2018-03-20 16:56:02,445] Average reward: 7.61 +/- 0.56\n",
      "[2018-03-20 16:56:02,797] Average reward: 6.88 +/- 0.56\n",
      "[2018-03-20 16:56:03,141] Average reward: 9.02 +/- 0.59\n",
      "[2018-03-20 16:56:03,491] Average reward: 10.32 +/- 0.60\n",
      "[2018-03-20 16:56:03,828] Average reward: 10.66 +/- 0.61\n",
      "[2018-03-20 16:56:04,164] Average reward: 10.46 +/- 0.59\n",
      "[2018-03-20 16:56:04,519] Average reward: 12.09 +/- 0.60\n",
      "[2018-03-20 16:56:04,911] Average reward: 12.27 +/- 0.59\n",
      "[2018-03-20 16:56:05,258] Average reward: 14.22 +/- 0.57\n",
      "[2018-03-20 16:56:05,625] Average reward: 14.44 +/- 0.56\n",
      "[2018-03-20 16:56:05,994] Average reward: 15.09 +/- 0.53\n",
      "[2018-03-20 16:56:06,373] Average reward: 14.89 +/- 0.53\n",
      "[2018-03-20 16:56:06,723] Average reward: 15.29 +/- 0.52\n",
      "[2018-03-20 16:56:07,078] Average reward: 15.25 +/- 0.52\n",
      "[2018-03-20 16:56:07,413] Average reward: 17.21 +/- 0.43\n",
      "[2018-03-20 16:56:07,414] - Training done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the state is :[ 1.  2.  0.  0.]\n",
      "the action is Count on\n",
      "At step 1 the state is :[ 1.  2.  1.  1.]\n",
      "the action is Count on\n",
      "At step 2 the state is :[ 1.  2.  2.  2.]\n",
      "the action is Count on\n",
      "At step 3 the state is :[ 1.  2.  3.  3.]\n",
      "the action is Done\n"
     ]
    }
   ],
   "source": [
    "env = EnvAdd(1,2)\n",
    "# train model\n",
    "model = PG(env, config)\n",
    "model.run()\n",
    "model.produce_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the state is :[ 1.  2.  0.  0.]\n",
      "the action is Count on\n",
      "At step 1 the state is :[ 1.  2.  1.  1.]\n",
      "the action is Count on\n",
      "At step 2 the state is :[ 1.  2.  2.  2.]\n",
      "the action is Count on\n",
      "At step 3 the state is :[ 1.  2.  3.  3.]\n",
      "the action is Done\n"
     ]
    }
   ],
   "source": [
    "model.produce_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.change_env(EnvAdd(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-20 16:32:06,950] Average reward: 12.08 +/- 0.80\n",
      "[2018-03-20 16:32:07,283] Average reward: 13.02 +/- 0.78\n",
      "[2018-03-20 16:32:07,620] Average reward: 15.31 +/- 0.70\n",
      "[2018-03-20 16:32:07,952] Average reward: 9.87 +/- 0.81\n",
      "[2018-03-20 16:32:08,298] Average reward: 10.59 +/- 0.81\n",
      "[2018-03-20 16:32:08,663] Average reward: 12.80 +/- 0.78\n",
      "[2018-03-20 16:32:09,067] Average reward: 14.86 +/- 0.73\n",
      "[2018-03-20 16:32:09,438] Average reward: 14.31 +/- 0.75\n",
      "[2018-03-20 16:32:09,790] Average reward: 9.93 +/- 0.81\n",
      "[2018-03-20 16:32:10,130] Average reward: 8.96 +/- 0.80\n",
      "[2018-03-20 16:32:10,458] Average reward: 9.93 +/- 0.81\n",
      "[2018-03-20 16:32:10,779] Average reward: 13.15 +/- 0.78\n",
      "[2018-03-20 16:32:11,124] Average reward: 12.05 +/- 0.80\n",
      "[2018-03-20 16:32:11,471] Average reward: 14.42 +/- 0.74\n",
      "[2018-03-20 16:32:11,812] Average reward: 14.76 +/- 0.73\n",
      "[2018-03-20 16:32:12,156] Average reward: 15.62 +/- 0.68\n",
      "[2018-03-20 16:32:12,481] Average reward: 12.80 +/- 0.78\n",
      "[2018-03-20 16:32:12,821] Average reward: 12.05 +/- 0.80\n",
      "[2018-03-20 16:32:13,160] Average reward: 9.87 +/- 0.81\n",
      "[2018-03-20 16:32:13,501] Average reward: 10.39 +/- 0.81\n",
      "[2018-03-20 16:32:13,845] Average reward: 12.30 +/- 0.80\n",
      "[2018-03-20 16:32:14,191] Average reward: 14.58 +/- 0.74\n",
      "[2018-03-20 16:32:14,542] Average reward: 14.34 +/- 0.75\n",
      "[2018-03-20 16:32:14,921] Average reward: 10.99 +/- 0.81\n",
      "[2018-03-20 16:32:15,350] Average reward: 11.58 +/- 0.80\n",
      "[2018-03-20 16:32:15,351] - Training done.\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the state is :[ 5.  1.  0.  0.]\n",
      "the action is Count on\n",
      "At step 1 the state is :[ 5.  1.  1.  1.]\n",
      "the action is Count on\n",
      "At step 2 the state is :[ 5.  1.  2.  2.]\n",
      "the action is Count on\n",
      "At step 3 the state is :[ 5.  1.  3.  3.]\n",
      "the action is Count on\n",
      "At step 4 the state is :[ 5.  1.  4.  4.]\n",
      "the action is Count on\n",
      "At step 5 the state is :[ 5.  1.  5.  5.]\n",
      "the action is Count on\n",
      "At step 6 the state is :[ 5.  1.  6.  6.]\n",
      "the action is Done\n"
     ]
    }
   ],
   "source": [
    "model.produce_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.change_env(EnvAdd(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the state is :[ 1.  3.  0.  0.]\n",
      "the action is Count on\n",
      "At step 1 the state is :[ 1.  3.  1.  1.]\n",
      "the action is Count on\n",
      "At step 2 the state is :[ 1.  3.  2.  2.]\n",
      "the action is Count on\n",
      "At step 3 the state is :[ 1.  3.  3.  3.]\n",
      "the action is Done\n"
     ]
    }
   ],
   "source": [
    "model.produce_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.change_env(EnvAdd2(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the state is :[ 1.  4.  0.  0.]\n",
      "the action is Count on\n",
      "At step 1 the state is :[ 1.  4.  1.  1.]\n",
      "the action is Count on\n",
      "At step 2 the state is :[ 1.  4.  2.  2.]\n",
      "the action is Count on\n",
      "At step 3 the state is :[ 1.  4.  3.  3.]\n",
      "the action is Count on\n",
      "At step 4 the state is :[ 1.  4.  4.  4.]\n",
      "the action is Count on\n",
      "At step 5 the state is :[ 1.  4.  5.  5.]\n",
      "the action is Done\n"
     ]
    }
   ],
   "source": [
    "model.produce_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
